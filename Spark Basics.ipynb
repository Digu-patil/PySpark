{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb485c1",
   "metadata": {},
   "source": [
    "## Building the Spark Session\n",
    "\n",
    "- Spark Sessions help us to create a spark application which helps us to communicate with the Driver (JVM)\n",
    "- Python Code -> Spark Session -> Driver\n",
    "- This Spark Session starts the driver process, setsup the SparkContect, and establishes the bridge to the JVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b219d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version :  3.5.5\n"
     ]
    }
   ],
   "source": [
    "# Importing the Spark Session module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating an application using the SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Spark Basics\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Check if the application is created successfully by checking the spark appliaction version \n",
    "\n",
    "print('Spark Version : ',spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34087fb",
   "metadata": {},
   "source": [
    "___\n",
    "## Locally Hosted Spark UI\n",
    "- When we create a spark session it locally hosts a web ui page at http://localhost:4040/ locaiton\n",
    "- We can check all the Job statuses, DAGs, Plans here\n",
    "\n",
    "## General Deployment methods of Spark Applications\n",
    "\n",
    "||Use Case|Your Computer|Data Center Cloud|\n",
    "|---|---|---|---|\n",
    "|Local|Testing|Driver and Executor||\n",
    "|Client|Development|Driver|Executor|\n",
    "|Cluster|Prodcution||Driver and Executor|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a6c56",
   "metadata": {},
   "source": [
    "___\n",
    "## Understanding Flow of Python Code to Spark Execution\n",
    "\n",
    "```mermaid\n",
    "\n",
    "flowchart LR\n",
    "\n",
    "t1[Python Code]\n",
    "t2[\"Py4J (Bridge between Python and JVM)\"]\n",
    "t3[\"JVM\n",
    "(-Runs actual computations\n",
    "-Manages Clusters\n",
    "-Does parallel processing)\"]\n",
    "\n",
    "t1-->t2-->t3\n",
    "```\n",
    "\n",
    "### Drivers and Executors\n",
    "- Drivers and Executors are all part of the JVM\n",
    "- When running on cluster, Driver can be on a different JVM and Executors can be on a different JVM\n",
    "\n",
    "### Driver\n",
    "- Driver is a Project Manager\n",
    "- As mentioned above we create \"Spark Session\" to setup this \"Driver\" and the JVM communication channel.\n",
    "- There can be only one \"Driver\" for each Spark Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f010db",
   "metadata": {},
   "source": [
    "___\n",
    "## Spark Context\n",
    "\n",
    "- Condier Spark Context as the brain of your Spark Application\n",
    "- It gives you information about every thing (all Spark setting that can affect your job)\n",
    "    1. The execution environment\n",
    "    2. Where are you running the Application (Locally/Cluster)\n",
    "    3. How many cores and memory is available \n",
    "    4. Driver and Executor info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5f56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have named our session as \"spark\" above \n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2cc0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Information :  local[*]\n",
      "App Name : Spark Basics \n",
      "Application Id : local-1745675883616\n",
      "Default Parallelism (Cores Used): 16\n",
      "Application Start Time (ms) : 1745675882092\n",
      "Application Start Time :  2025-04-26 19:28:02.092000\n"
     ]
    }
   ],
   "source": [
    "# Let's try to answer a few questions\n",
    "\n",
    "# 1. Where are we running Spark? and how many cores are available to us?\n",
    "print(\"Master Information : \", sc.master)\n",
    "\n",
    "'''\n",
    "If sc.master returns \n",
    "1. local --> then its only using 1 core\n",
    "2. local[n] --> then it is using n threads/cores\n",
    "3. local[*] --> then it is using all the avaliable logical cores (read physical vs logic core page for more info - link)\n",
    "4. YARN/Mesos/Kubernetes --> when you use an external cluster manager\n",
    "'''\n",
    "\n",
    "# 2. If you want to know the App Name and its Id you gave while starting the setting \n",
    "print(f'App Name : {sc.appName} \\nApplication Id : {sc.applicationId}')\n",
    "\n",
    "# 3. If you want to konw how many cores are available for Spark to executre its tasks (Note that these will be logical cores)\n",
    "print(\"Default Parallelism (Cores Used):\", sc.defaultParallelism)\n",
    "\n",
    "# 4. Applciation start time deatils\n",
    "print(f'Application Start Time (ms) : {sc.startTime}' ) \n",
    "\n",
    "'''\n",
    "startTime will return us a value in milliseconds from the Unix epoch (January 1, 1970)\n",
    "we will need to conver this to normal date\n",
    "'''\n",
    "import datetime as dt\n",
    "print(\"Application Start Time : \", dt.datetime.fromtimestamp(sc.startTime/1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fffe007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
